{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from natsort import natsorted as nsort\n",
    "import pandas as pd\n",
    "from logger.test_log import load_test_log\n",
    "from logger.evaluation_log import load_eval_log\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_rows(s, column, color, min = None, max = None):\n",
    "    is_fit = pd.Series(data=False, index=s.index)\n",
    "    is_fit[column] = True\n",
    "\n",
    "    if min is not None:\n",
    "        is_fit[column] &= s.loc[column] >= min\n",
    "    \n",
    "    if max is not None:\n",
    "        is_fit[column] &= s.loc[column] < max\n",
    "        \n",
    "    return [f'background-color: {color}' if is_fit.any() else '' for v in is_fit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CS = '10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Test Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base_Algorithms = [\"PPO\", \n",
    "              \"PPO_CAPG\",\n",
    "              \"PPO_MAXA\", \n",
    "              \"PPO_TANH\", \n",
    "              \"SB3_TRPO\",\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "Algorithms = []\n",
    "\n",
    "for base in Base_Algorithms:\n",
    "    Algorithms.append(base)\n",
    "    Algorithms.append(base + \"_USER\")\n",
    "    Algorithms.append(base + \"_GRID\")\n",
    "    Algorithms.append(base + \"_TOTAL_SUM\")\n",
    "    Algorithms.append(base + \"_TOTAL_MIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Algorithm\",\n",
    "           \"Testing environment\",\n",
    "           \"Reward(e3)\",\n",
    "           \"Profits / Costs(€)\", \n",
    "           \"User satisfaction(%)\",\n",
    "           \"Energy Charged(kWh)\",\n",
    "           \"Energy Discharged(kWh)\",\n",
    "           \"Transformer Overload(kWh)\",\n",
    "           \"Total Battery Capacity Loss(e-3)\",\n",
    "           \"Total Battery Degradation Calender(e-3)\",\n",
    "           \"Total Battery Degradation Cycle(e-3)\",\n",
    "           \"Execution Time of episodes(s)\",\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = []\n",
    "max_rewards = []\n",
    "\n",
    "for algo in Algorithms:\n",
    "    file = nsort(os.listdir(f\"log/test/{N_CS}/{algo}\"))[-1]\n",
    "    if file.endswith(\".pkl\"):\n",
    "        test_logs = load_test_log(f\"log/test/{N_CS}/{algo}/{file}\")\n",
    "        for i in range(len(test_logs)):\n",
    "\n",
    "            if len(max_rewards) <= i:\n",
    "                max_rewards.append([])\n",
    "\n",
    "            max_rewards[i].append(test_logs[i][\"reward\"])\n",
    "\n",
    "            if len(test_array) <= i: \n",
    "                test_array.append([])\n",
    "\n",
    "            if len(max_rewards[i]) > 1:\n",
    "                max_rewards[i] = sorted(max_rewards[i], reverse=True)[:3]\n",
    "                \n",
    "            test_array[i].append({\n",
    "                \"Algorithm\": algo,\n",
    "                \"Testing environment\": i+1,\n",
    "                \"Profits / Costs(€)\": test_logs[i][\"profits\"],\n",
    "                \"User satisfaction(%)\": test_logs[i][\"user_satisfaction\"],\n",
    "                \"Energy Charged(kWh)\": test_logs[i][\"energy_charged\"],\n",
    "                \"Energy Discharged(kWh)\": test_logs[i][\"energy_discharged\"],\n",
    "                \"Transformer Overload(kWh)\": test_logs[i][\"transformer_overload\"],\n",
    "                \"Total Battery Capacity Loss(e-3)\": test_logs[i][\"battery_degradation\"],\n",
    "                \"Total Battery Degradation Calender(e-3)\": test_logs[i][\"battery_degradation_calendar\"],\n",
    "                \"Total Battery Degradation Cycle(e-3)\": test_logs[i][\"battery_degradation_cycling\"],\n",
    "                \"Execution Time of episodes(s)\": test_logs[i][\"execution_time\"],\n",
    "                \"Reward(e3)\": test_logs[i][\"reward\"],\n",
    "            })\n",
    "\n",
    "test_dfs = []\n",
    "\n",
    "for i, test_logs in enumerate(test_array):\n",
    "    test_dfs.append(\n",
    "        pd.DataFrame(test_logs, \n",
    "                     columns=columns\n",
    "                     ).style.apply(highlight_rows, \n",
    "                                   min=max_rewards[i][0],\n",
    "                                   column='Reward(e3)', \n",
    "                                   color=\"#0000CC\", \n",
    "                                   axis=1)\\\n",
    "                      .apply(highlight_rows,\n",
    "                                   min=max_rewards[i][1],\n",
    "                                   max=max_rewards[i][0],\n",
    "                                   column='Reward(e3)', \n",
    "                                   color=\"#000099\", \n",
    "                                   axis=1)\\\n",
    "                      .apply(highlight_rows,\n",
    "                                   min=max_rewards[i][2],\n",
    "                                   max=max_rewards[i][1],\n",
    "                                   column='Reward(e3)', \n",
    "                                   color=\"#000066\", \n",
    "                                   axis=1)\n",
    "                    )\n",
    "\n",
    "for i, test_df in enumerate(test_dfs):\n",
    "    display(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_evals = []\n",
    "\n",
    "for algo in Algorithms:\n",
    "    file = nsort(os.listdir(f\"log/eval/{N_CS}/{algo}\"))[-1]\n",
    "    if file.endswith(\".pkl\"):\n",
    "        eval_logs = load_eval_log(f\"log/eval/{N_CS}/{algo}/{file}\")\n",
    "        algorithm_evals.append(eval_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rewards_graphs(rewards_list, title=\"Title\", group_size=10, skip=0, ignore_list=None, interactive=True):\n",
    "    if ignore_list is None:\n",
    "        ignore_list = []\n",
    "\n",
    "    # Generate a fixed color map for algorithms\n",
    "    color_cycle = plt.cm.tab10.colors  # Use a colormap with 10 distinct colors\n",
    "    algorithm_colors = {name: color for name, color in zip([r[1] for r in rewards_list], itertools.cycle(color_cycle))}\n",
    "\n",
    "    def plot_graph(group_size, skip, ignore_list):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.ylabel('reward')\n",
    "        plt.xlabel(title)\n",
    "        plt.title(title)\n",
    "\n",
    "        for rewards in rewards_list:\n",
    "            if rewards[1] in ignore_list:\n",
    "                continue\n",
    "            values = rewards[0][skip:]\n",
    "            color = algorithm_colors[rewards[1]]  # Get the fixed color for the algorithm\n",
    "            plt.plot([sum(values[i * group_size: (i + 1) * group_size]) / group_size for i in range(int(len(values) / group_size))], \n",
    "                     label=rewards[1], color=color)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    if interactive:\n",
    "        group_size_slider = widgets.IntSlider(value=group_size, min=1, max=100, step=1, description='Group Size')\n",
    "        skip_slider = widgets.IntSlider(value=skip, min=0, max=100, step=1, description='Skip')\n",
    "\n",
    "        # Checkbox creation\n",
    "        checkboxes = []\n",
    "        for _, name in rewards_list:\n",
    "            checkboxes.append(widgets.Checkbox(value=True, description=name))\n",
    "\n",
    "        # Arrange checkboxes in rows of 4\n",
    "        checkbox_grid = []\n",
    "        for i in range(0, len(checkboxes), 4):\n",
    "            checkbox_grid.append(widgets.HBox(checkboxes[i:i+4]))\n",
    "\n",
    "        def update_plot(group_size, skip, **kwargs):\n",
    "            current_ignore_list = [name for name, value in kwargs.items() if not value]\n",
    "            plot_graph(group_size, skip, current_ignore_list)\n",
    "\n",
    "        checkbox_dict = {checkbox.description: checkbox for checkbox in checkboxes}\n",
    "        ui = widgets.VBox([group_size_slider, skip_slider, widgets.VBox(checkbox_grid)])\n",
    "\n",
    "        out = widgets.interactive_output(update_plot, {'group_size': group_size_slider, 'skip': skip_slider, **checkbox_dict})\n",
    "\n",
    "        display(ui, out)\n",
    "    else:\n",
    "        plot_graph(group_size, skip, ignore_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_rewards_graphs(algorithm_evals, title=\"Evaluation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ev2gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
